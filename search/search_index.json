{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"User Guide Quick links: \ud83d\udcbb GitHub Repository \ud83d\udcda Documentation / Readthedocs \ud83d\udc0d PyPi project \ud83e\uddea Colab Demo / Kaggle Demo Quickstart Install the library with pip: pip install dl-translate To translate some text: import dl_translate as dlt mt = dlt . TranslationModel () # Slow when you load it for the first time text_hi = \"\u0938\u0902\u092f\u0941\u0915\u094d\u0924 \u0930\u093e\u0937\u094d\u091f\u094d\u0930 \u0915\u0947 \u092a\u094d\u0930\u092e\u0941\u0916 \u0915\u093e \u0915\u0939\u0928\u093e \u0939\u0948 \u0915\u093f \u0938\u0940\u0930\u093f\u092f\u093e \u092e\u0947\u0902 \u0915\u094b\u0908 \u0938\u0948\u0928\u094d\u092f \u0938\u092e\u093e\u0927\u093e\u0928 \u0928\u0939\u0940\u0902 \u0939\u0948\" mt . translate ( text_hi , source = dlt . lang . HINDI , target = dlt . lang . ENGLISH ) Above, you can see that dlt.lang contains variables representing each of the 50 available languages with auto-complete support. Alternatively, you can specify the language (e.g. \"Arabic\") or the language code (e.g. \"fr\" for French): text_ar = \"\u0627\u0644\u0623\u0645\u064a\u0646 \u0627\u0644\u0639\u0627\u0645 \u0644\u0644\u0623\u0645\u0645 \u0627\u0644\u0645\u062a\u062d\u062f\u0629 \u064a\u0642\u0648\u0644 \u0625\u0646\u0647 \u0644\u0627 \u064a\u0648\u062c\u062f \u062d\u0644 \u0639\u0633\u0643\u0631\u064a \u0641\u064a \u0633\u0648\u0631\u064a\u0627.\" mt . translate ( text_ar , source = \"Arabic\" , target = \"fr\" ) If you want to verify whether a language is available, you can check it: print ( mt . available_languages ()) # All languages that you can use print ( mt . available_codes ()) # Code corresponding to each language accepted print ( mt . get_lang_code_map ()) # Dictionary of lang -> code Usage Selecting a device When you load the model, you can specify the device using the device argument. By default, the value will be device=\"auto\" , which means it will use a GPU if possible. You can also explicitly set device=\"cpu\" or device=\"gpu\" , or some other strings accepted by torch.device() . In general, it is recommend to use a GPU if you want a reasonable processing time. mt = dlt . TranslationModel ( device = \"auto\" ) # Automatically select device mt = dlt . TranslationModel ( device = \"cpu\" ) # Force you to use a CPU mt = dlt . TranslationModel ( device = \"gpu\" ) # Force you to use a GPU mt = dlt . TranslationModel ( device = \"cuda:2\" ) # Use the 3rd GPU available Changing the model you are loading Two model families are available at the moment: m2m100 and mBART-50 Large , which respective allow translation across over 100 languages and 50 languages. By default, the model will select m2m100 , but you can also explicitly choose the model by specifying the shorthand ( \"m2m100\" or \"mbart50\" ) or the full repository name (e.g. \"facebook/m2m100_418M\" ). For example: # The following ways are equivalent mt = dlt . TranslationModel ( \"m2m100\" ) # Default mt = dlt . TranslationModel ( \"facebook/m2m100_418M\" ) # The following ways are equivalent mt = dlt . TranslationModel ( \"mbart50\" ) mt = dlt . TranslationModel ( \"facebook/mbart-large-50-many-to-many-mmt\" ) Note that the language code will change depending on the model family. To find out the correct language codes, please read the doc page on available languages or run mt.available_codes() . Loading from a path By default, dlt.TranslationModel will download the model from the huggingface repo and cache it. If your model is stored locally, you can also directly load that model, but in that case you will need to specify the model family (e.g. \"mbart50\" and \"m2m100\" ). mt = dlt . TranslationModel ( \"/path/to/model/directory/\" , model_family = \"mbart50\" ) # or mt = dlt . TranslationModel ( \"/path/to/model/directory/\" , model_family = \"m2m100\" ) Make sure that your tokenizer is also stored in the same directory if you use this approach. Using a different model You can also choose another model that has the same format as mbart50 or m2m100 e.g. mt = dlt . TranslationModel ( \"facebook/mbart-large-50-one-to-many-mmt\" , model_family = \"mbart50\" ) # or mt = dlt . TranslationModel ( \"facebook/m2m100_1.2B\" , model_family = \"m2m100\" ) Note that the available languages will change if you do this, so you will not be able to leverage dlt.lang or dlt.utils and the mt.available_languages() might also return the incorrect value. Breaking down into sentences It is not recommended to use extremely long texts as it takes more time to process. Instead, you can try to break them down into sentences. Multiple solutions exists for that, including doing it manually and using the nltk library. A quick approach would be to split them by period. However, you have to ensure that there are no periods used for abbreviations (such as Mr. or Dr. ). For example, it will work in the following case: text = \"Mr Smith went to his favorite cafe. There, he met his friend Dr Doe.\" sents = text . split ( \".\" ) \".\" . join ( mt . translate ( sents , source = dlt . lang . ENGLISH , target = dlt . lang . FRENCH )) For more complex cases (e.g. where you use periods for abbreviations), you can use nltk . First install the library with pip install nltk , then run: import nltk nltk . download ( \"punkt\" ) text = \"Mr. Smith went to his favorite cafe. There, he met his friend Dr. Doe.\" sents = nltk . tokenize . sent_tokenize ( text , \"english\" ) # don't use dlt.lang.ENGLISH \" \" . join ( mt . translate ( sents , source = dlt . lang . ENGLISH , target = dlt . lang . FRENCH )) Batch size and verbosity when using translate It's possible to set a batch size (i.e. the number of elements processed at once) for mt.translate and whether you want to see the progress bar or not: ... mt = dlt . TranslationModel () mt . translate ( text , source , target , batch_size = 32 , verbose = True ) If you set batch_size=None , it will compute the entire text at once rather than splitting into \"chunks\". We recommend lowering batch_size if you do not have a lot of RAM or VRAM and run into CUDA memory error. Set a higher value if you are using a high-end GPU and the VRAM is not fully utilized. dlt.utils module An alternative to mt.available_languages() is the dlt.utils module. You can use it to find out which languages and codes are available: print ( dlt . utils . available_languages ( 'mbart50' )) # All languages that you can use print ( dlt . utils . available_codes ( 'mbart50' )) # Code corresponding to each language accepted print ( dlt . utils . get_lang_code_map ( 'mbart50' )) # Dictionary of lang -> code print ( dlt . utils . available_languages ( 'm2m100' )) # write the name of the model family At the moment, the following models are accepted: - \"mbart50\" - \"m2m100\" Offline usage Unlike the Google translate or MSFT Translator APIs, this library can be fully used offline. However, you will need to first download the packages and models, and move them to your offline environment to be installed and loaded inside a venv. First, run in your terminal: mkdir dlt cd dlt mkdir libraries pip download -d libraries/ dl-translate Once all the required packages are downloaded, you will need to use huggingface hub to download the files. Install it with pip install huggingface-hub . Then, run inside Python: import os import huggingface_hub as hub dirname = hub . snapshot_download ( \"facebook/m2m100_418M\" ) os . rename ( dirname , \"cached_model_m2m100\" ) Now, move everything in the dlt directory to your offline environment. Create a virtual environment and run the following in terminal: pip install --no-index --find-links libraries/ dl-translate Now, run inside Python: import dl_translate as dlt mt = dlt . TranslationModel ( \"cached_model_m2m100\" , model_family = \"m2m100\" ) Advanced The following section assumes you have knowledge of PyTorch and Huggingface Transformers. Saving and loading If you wish to accelerate the loading time the translation model, you can use save_obj . Later you can reload it with load_obj by specifying the same directory that you are using to save. mt = dlt . TranslationModel () # ... mt . save_obj ( 'saved_model' ) # ... mt = dlt . TranslationModel . load_obj ( 'saved_model' ) Warning: Only use this if you are certain the torch module saved in saved_model/weights.pt can be correctly loaded. Indeed, it is possible that the huggingface , torch or some other dependencies change between when you called save_obj and load_obj , and that might break your code. Thus, it is recommend to only run load_obj in the same environment/session as save_obj . Note this method might be deprecated in the future once there's no speed benefit in loading this way. Interacting with underlying model and tokenizer When initializing model , you can pass in arguments for the underlying BART model and tokenizer (which will respectively be passed to ModelForConditionalGeneration.from_pretrained and TokenizerFast.from_pretrained ): mt = dlt . TranslationModel ( model_options = dict ( state_dict =... , cache_dir =... , ... ), tokenizer_options = dict ( tokenizer_file =... , eos_token =... , ... ) ) You can also access the underlying transformers model and tokenizer : transformers_model = mt . get_transformers_model () tokenizer = mt . get_tokenizer () For more information about the models themselves, please read the docs on mBART and m2m100 . Keyword arguments for the generate() method of the underlying model When running mt.translate , you can also give a generation_options dictionary that is passed as keyword arguments to the underlying mt.get_transformers_model().generate() method: mt . translate ( text , source = dlt . lang . GERMAN , target = dlt . lang . SPANISH , generation_options = dict ( num_beams = 5 , max_length =... ) ) Learn more in the huggingface docs .","title":"User Guide"},{"location":"#user-guide","text":"Quick links: \ud83d\udcbb GitHub Repository \ud83d\udcda Documentation / Readthedocs \ud83d\udc0d PyPi project \ud83e\uddea Colab Demo / Kaggle Demo","title":"User Guide"},{"location":"#quickstart","text":"Install the library with pip: pip install dl-translate To translate some text: import dl_translate as dlt mt = dlt . TranslationModel () # Slow when you load it for the first time text_hi = \"\u0938\u0902\u092f\u0941\u0915\u094d\u0924 \u0930\u093e\u0937\u094d\u091f\u094d\u0930 \u0915\u0947 \u092a\u094d\u0930\u092e\u0941\u0916 \u0915\u093e \u0915\u0939\u0928\u093e \u0939\u0948 \u0915\u093f \u0938\u0940\u0930\u093f\u092f\u093e \u092e\u0947\u0902 \u0915\u094b\u0908 \u0938\u0948\u0928\u094d\u092f \u0938\u092e\u093e\u0927\u093e\u0928 \u0928\u0939\u0940\u0902 \u0939\u0948\" mt . translate ( text_hi , source = dlt . lang . HINDI , target = dlt . lang . ENGLISH ) Above, you can see that dlt.lang contains variables representing each of the 50 available languages with auto-complete support. Alternatively, you can specify the language (e.g. \"Arabic\") or the language code (e.g. \"fr\" for French): text_ar = \"\u0627\u0644\u0623\u0645\u064a\u0646 \u0627\u0644\u0639\u0627\u0645 \u0644\u0644\u0623\u0645\u0645 \u0627\u0644\u0645\u062a\u062d\u062f\u0629 \u064a\u0642\u0648\u0644 \u0625\u0646\u0647 \u0644\u0627 \u064a\u0648\u062c\u062f \u062d\u0644 \u0639\u0633\u0643\u0631\u064a \u0641\u064a \u0633\u0648\u0631\u064a\u0627.\" mt . translate ( text_ar , source = \"Arabic\" , target = \"fr\" ) If you want to verify whether a language is available, you can check it: print ( mt . available_languages ()) # All languages that you can use print ( mt . available_codes ()) # Code corresponding to each language accepted print ( mt . get_lang_code_map ()) # Dictionary of lang -> code","title":"Quickstart"},{"location":"#usage","text":"","title":"Usage"},{"location":"#selecting-a-device","text":"When you load the model, you can specify the device using the device argument. By default, the value will be device=\"auto\" , which means it will use a GPU if possible. You can also explicitly set device=\"cpu\" or device=\"gpu\" , or some other strings accepted by torch.device() . In general, it is recommend to use a GPU if you want a reasonable processing time. mt = dlt . TranslationModel ( device = \"auto\" ) # Automatically select device mt = dlt . TranslationModel ( device = \"cpu\" ) # Force you to use a CPU mt = dlt . TranslationModel ( device = \"gpu\" ) # Force you to use a GPU mt = dlt . TranslationModel ( device = \"cuda:2\" ) # Use the 3rd GPU available","title":"Selecting a device"},{"location":"#changing-the-model-you-are-loading","text":"Two model families are available at the moment: m2m100 and mBART-50 Large , which respective allow translation across over 100 languages and 50 languages. By default, the model will select m2m100 , but you can also explicitly choose the model by specifying the shorthand ( \"m2m100\" or \"mbart50\" ) or the full repository name (e.g. \"facebook/m2m100_418M\" ). For example: # The following ways are equivalent mt = dlt . TranslationModel ( \"m2m100\" ) # Default mt = dlt . TranslationModel ( \"facebook/m2m100_418M\" ) # The following ways are equivalent mt = dlt . TranslationModel ( \"mbart50\" ) mt = dlt . TranslationModel ( \"facebook/mbart-large-50-many-to-many-mmt\" ) Note that the language code will change depending on the model family. To find out the correct language codes, please read the doc page on available languages or run mt.available_codes() .","title":"Changing the model you are loading"},{"location":"#loading-from-a-path","text":"By default, dlt.TranslationModel will download the model from the huggingface repo and cache it. If your model is stored locally, you can also directly load that model, but in that case you will need to specify the model family (e.g. \"mbart50\" and \"m2m100\" ). mt = dlt . TranslationModel ( \"/path/to/model/directory/\" , model_family = \"mbart50\" ) # or mt = dlt . TranslationModel ( \"/path/to/model/directory/\" , model_family = \"m2m100\" ) Make sure that your tokenizer is also stored in the same directory if you use this approach.","title":"Loading from a path"},{"location":"#using-a-different-model","text":"You can also choose another model that has the same format as mbart50 or m2m100 e.g. mt = dlt . TranslationModel ( \"facebook/mbart-large-50-one-to-many-mmt\" , model_family = \"mbart50\" ) # or mt = dlt . TranslationModel ( \"facebook/m2m100_1.2B\" , model_family = \"m2m100\" ) Note that the available languages will change if you do this, so you will not be able to leverage dlt.lang or dlt.utils and the mt.available_languages() might also return the incorrect value.","title":"Using a different model"},{"location":"#breaking-down-into-sentences","text":"It is not recommended to use extremely long texts as it takes more time to process. Instead, you can try to break them down into sentences. Multiple solutions exists for that, including doing it manually and using the nltk library. A quick approach would be to split them by period. However, you have to ensure that there are no periods used for abbreviations (such as Mr. or Dr. ). For example, it will work in the following case: text = \"Mr Smith went to his favorite cafe. There, he met his friend Dr Doe.\" sents = text . split ( \".\" ) \".\" . join ( mt . translate ( sents , source = dlt . lang . ENGLISH , target = dlt . lang . FRENCH )) For more complex cases (e.g. where you use periods for abbreviations), you can use nltk . First install the library with pip install nltk , then run: import nltk nltk . download ( \"punkt\" ) text = \"Mr. Smith went to his favorite cafe. There, he met his friend Dr. Doe.\" sents = nltk . tokenize . sent_tokenize ( text , \"english\" ) # don't use dlt.lang.ENGLISH \" \" . join ( mt . translate ( sents , source = dlt . lang . ENGLISH , target = dlt . lang . FRENCH ))","title":"Breaking down into sentences"},{"location":"#batch-size-and-verbosity-when-using-translate","text":"It's possible to set a batch size (i.e. the number of elements processed at once) for mt.translate and whether you want to see the progress bar or not: ... mt = dlt . TranslationModel () mt . translate ( text , source , target , batch_size = 32 , verbose = True ) If you set batch_size=None , it will compute the entire text at once rather than splitting into \"chunks\". We recommend lowering batch_size if you do not have a lot of RAM or VRAM and run into CUDA memory error. Set a higher value if you are using a high-end GPU and the VRAM is not fully utilized.","title":"Batch size and verbosity when using translate"},{"location":"#dltutils-module","text":"An alternative to mt.available_languages() is the dlt.utils module. You can use it to find out which languages and codes are available: print ( dlt . utils . available_languages ( 'mbart50' )) # All languages that you can use print ( dlt . utils . available_codes ( 'mbart50' )) # Code corresponding to each language accepted print ( dlt . utils . get_lang_code_map ( 'mbart50' )) # Dictionary of lang -> code print ( dlt . utils . available_languages ( 'm2m100' )) # write the name of the model family At the moment, the following models are accepted: - \"mbart50\" - \"m2m100\"","title":"dlt.utils module"},{"location":"#offline-usage","text":"Unlike the Google translate or MSFT Translator APIs, this library can be fully used offline. However, you will need to first download the packages and models, and move them to your offline environment to be installed and loaded inside a venv. First, run in your terminal: mkdir dlt cd dlt mkdir libraries pip download -d libraries/ dl-translate Once all the required packages are downloaded, you will need to use huggingface hub to download the files. Install it with pip install huggingface-hub . Then, run inside Python: import os import huggingface_hub as hub dirname = hub . snapshot_download ( \"facebook/m2m100_418M\" ) os . rename ( dirname , \"cached_model_m2m100\" ) Now, move everything in the dlt directory to your offline environment. Create a virtual environment and run the following in terminal: pip install --no-index --find-links libraries/ dl-translate Now, run inside Python: import dl_translate as dlt mt = dlt . TranslationModel ( \"cached_model_m2m100\" , model_family = \"m2m100\" )","title":"Offline usage"},{"location":"#advanced","text":"The following section assumes you have knowledge of PyTorch and Huggingface Transformers.","title":"Advanced"},{"location":"#saving-and-loading","text":"If you wish to accelerate the loading time the translation model, you can use save_obj . Later you can reload it with load_obj by specifying the same directory that you are using to save. mt = dlt . TranslationModel () # ... mt . save_obj ( 'saved_model' ) # ... mt = dlt . TranslationModel . load_obj ( 'saved_model' ) Warning: Only use this if you are certain the torch module saved in saved_model/weights.pt can be correctly loaded. Indeed, it is possible that the huggingface , torch or some other dependencies change between when you called save_obj and load_obj , and that might break your code. Thus, it is recommend to only run load_obj in the same environment/session as save_obj . Note this method might be deprecated in the future once there's no speed benefit in loading this way.","title":"Saving and loading"},{"location":"#interacting-with-underlying-model-and-tokenizer","text":"When initializing model , you can pass in arguments for the underlying BART model and tokenizer (which will respectively be passed to ModelForConditionalGeneration.from_pretrained and TokenizerFast.from_pretrained ): mt = dlt . TranslationModel ( model_options = dict ( state_dict =... , cache_dir =... , ... ), tokenizer_options = dict ( tokenizer_file =... , eos_token =... , ... ) ) You can also access the underlying transformers model and tokenizer : transformers_model = mt . get_transformers_model () tokenizer = mt . get_tokenizer () For more information about the models themselves, please read the docs on mBART and m2m100 .","title":"Interacting with underlying model and tokenizer"},{"location":"#keyword-arguments-for-the-generate-method-of-the-underlying-model","text":"When running mt.translate , you can also give a generation_options dictionary that is passed as keyword arguments to the underlying mt.get_transformers_model().generate() method: mt . translate ( text , source = dlt . lang . GERMAN , target = dlt . lang . SPANISH , generation_options = dict ( num_beams = 5 , max_length =... ) ) Learn more in the huggingface docs .","title":"Keyword arguments for the generate() method of the underlying model"},{"location":"available_languages/","text":"Languages Available This page gives all the languages available for each model family. MBart 50 Arabic (ar_AR) Czech (cs_CZ) German (de_DE) English (en_XX) Spanish (es_XX) Estonian (et_EE) Finnish (fi_FI) French (fr_XX) Gujarati (gu_IN) Hindi (hi_IN) Italian (it_IT) Japanese (ja_XX) Kazakh (kk_KZ) Korean (ko_KR) Lithuanian (lt_LT) Latvian (lv_LV) Burmese (my_MM) Nepali (ne_NP) Dutch (nl_XX) Romanian (ro_RO) Russian (ru_RU) Sinhala (si_LK) Turkish (tr_TR) Vietnamese (vi_VN) Chinese (zh_CN) Afrikaans (af_ZA) Azerbaijani (az_AZ) Bengali (bn_IN) Persian (fa_IR) Hebrew (he_IL) Croatian (hr_HR) Indonesian (id_ID) Georgian (ka_GE) Khmer (km_KH) Macedonian (mk_MK) Malayalam (ml_IN) Mongolian (mn_MN) Marathi (mr_IN) Polish (pl_PL) Pashto (ps_AF) Portuguese (pt_XX) Swedish (sv_SE) Swahili (sw_KE) Tamil (ta_IN) Telugu (te_IN) Thai (th_TH) Tagalog (tl_XX) Ukrainian (uk_UA) Urdu (ur_PK) Xhosa (xh_ZA) Galician (gl_ES) Slovene (sl_SI) M2M-100 Afrikaans (af) Amharic (am) Arabic (ar) Asturian (ast) Azerbaijani (az) Bashkir (ba) Belarusian (be) Bulgarian (bg) Bengali (bn) Breton (br) Bosnian (bs) Catalan (ca) Valencian (ca) Cebuano (ceb) Czech (cs) Welsh (cy) Danish (da) German (de) Greek (el) English (en) Spanish (es) Estonian (et) Persian (fa) Fulah (ff) Finnish (fi) French (fr) Western Frisian (fy) Irish (ga) Gaelic (gd) Scottish Gaelic (gd) Galician (gl) Gujarati (gu) Hausa (ha) Hebrew (he) Hindi (hi) Croatian (hr) Haitian (ht) Haitian Creole (ht) Hungarian (hu) Armenian (hy) Indonesian (id) Igbo (ig) Iloko (ilo) Icelandic (is) Italian (it) Japanese (ja) Javanese (jv) Georgian (ka) Kazakh (kk) Khmer (km) Central Khmer (km) Kannada (kn) Korean (ko) Luxembourgish (lb) Letzeburgesch (lb) Ganda (lg) Lingala (ln) Lao (lo) Lithuanian (lt) Latvian (lv) Malagasy (mg) Macedonian (mk) Malayalam (ml) Mongolian (mn) Marathi (mr) Malay (ms) Burmese (my) Nepali (ne) Dutch (nl) Flemish (nl) Norwegian (no) Northern Sotho (ns) Occitan (oc) Oriya (or) Panjabi (pa) Punjabi (pa) Polish (pl) Pushto (ps) Pashto (ps) Portuguese (pt) Romanian (ro) Moldavian (ro) Moldovan (ro) Russian (ru) Sindhi (sd) Sinhala (si) Sinhalese (si) Slovak (sk) Slovenian (sl) Somali (so) Albanian (sq) Serbian (sr) Swati (ss) Sundanese (su) Swedish (sv) Swahili (sw) Tamil (ta) Thai (th) Tagalog (tl) Tswana (tn) Turkish (tr) Ukrainian (uk) Urdu (ur) Uzbek (uz) Vietnamese (vi) Wolof (wo) Xhosa (xh) Yiddish (yi) Yoruba (yo) Chinese (zh) Zulu (zu)","title":"Languages Available"},{"location":"available_languages/#languages-available","text":"This page gives all the languages available for each model family.","title":"Languages Available"},{"location":"available_languages/#mbart-50","text":"Arabic (ar_AR) Czech (cs_CZ) German (de_DE) English (en_XX) Spanish (es_XX) Estonian (et_EE) Finnish (fi_FI) French (fr_XX) Gujarati (gu_IN) Hindi (hi_IN) Italian (it_IT) Japanese (ja_XX) Kazakh (kk_KZ) Korean (ko_KR) Lithuanian (lt_LT) Latvian (lv_LV) Burmese (my_MM) Nepali (ne_NP) Dutch (nl_XX) Romanian (ro_RO) Russian (ru_RU) Sinhala (si_LK) Turkish (tr_TR) Vietnamese (vi_VN) Chinese (zh_CN) Afrikaans (af_ZA) Azerbaijani (az_AZ) Bengali (bn_IN) Persian (fa_IR) Hebrew (he_IL) Croatian (hr_HR) Indonesian (id_ID) Georgian (ka_GE) Khmer (km_KH) Macedonian (mk_MK) Malayalam (ml_IN) Mongolian (mn_MN) Marathi (mr_IN) Polish (pl_PL) Pashto (ps_AF) Portuguese (pt_XX) Swedish (sv_SE) Swahili (sw_KE) Tamil (ta_IN) Telugu (te_IN) Thai (th_TH) Tagalog (tl_XX) Ukrainian (uk_UA) Urdu (ur_PK) Xhosa (xh_ZA) Galician (gl_ES) Slovene (sl_SI)","title":"MBart 50"},{"location":"available_languages/#m2m-100","text":"Afrikaans (af) Amharic (am) Arabic (ar) Asturian (ast) Azerbaijani (az) Bashkir (ba) Belarusian (be) Bulgarian (bg) Bengali (bn) Breton (br) Bosnian (bs) Catalan (ca) Valencian (ca) Cebuano (ceb) Czech (cs) Welsh (cy) Danish (da) German (de) Greek (el) English (en) Spanish (es) Estonian (et) Persian (fa) Fulah (ff) Finnish (fi) French (fr) Western Frisian (fy) Irish (ga) Gaelic (gd) Scottish Gaelic (gd) Galician (gl) Gujarati (gu) Hausa (ha) Hebrew (he) Hindi (hi) Croatian (hr) Haitian (ht) Haitian Creole (ht) Hungarian (hu) Armenian (hy) Indonesian (id) Igbo (ig) Iloko (ilo) Icelandic (is) Italian (it) Japanese (ja) Javanese (jv) Georgian (ka) Kazakh (kk) Khmer (km) Central Khmer (km) Kannada (kn) Korean (ko) Luxembourgish (lb) Letzeburgesch (lb) Ganda (lg) Lingala (ln) Lao (lo) Lithuanian (lt) Latvian (lv) Malagasy (mg) Macedonian (mk) Malayalam (ml) Mongolian (mn) Marathi (mr) Malay (ms) Burmese (my) Nepali (ne) Dutch (nl) Flemish (nl) Norwegian (no) Northern Sotho (ns) Occitan (oc) Oriya (or) Panjabi (pa) Punjabi (pa) Polish (pl) Pushto (ps) Pashto (ps) Portuguese (pt) Romanian (ro) Moldavian (ro) Moldovan (ro) Russian (ru) Sindhi (sd) Sinhala (si) Sinhalese (si) Slovak (sk) Slovenian (sl) Somali (so) Albanian (sq) Serbian (sr) Swati (ss) Sundanese (su) Swedish (sv) Swahili (sw) Tamil (ta) Thai (th) Tagalog (tl) Tswana (tn) Turkish (tr) Ukrainian (uk) Urdu (ur) Uzbek (uz) Vietnamese (vi) Wolof (wo) Xhosa (xh) Yiddish (yi) Yoruba (yo) Chinese (zh) Zulu (zu)","title":"M2M-100"},{"location":"contributing/","text":"Contributions If you wish to contribute to the project, please do the following: 1. Verify if there's an existing similar issue. 2. If no issue exists, create it. 3. Once the contribution has been discussed inside the issue, fork this repo. 4. Before modifying any code, make sure to read the sections below. 5. Once you are done with your contribution, start a PR and tag a codeowner. Setup To set up the development environment, clone the repo: git clone https://github.com/xhlulu/dl-translate cd dl-translate Create a new venv and install the dev dependencies python -m venv venv source venv/bin/activate pip install -e . [ dev ] Code linting To ensure consistent and readable code, we use black . To run: python black . Running tests To run all the tests: python -m pytest tests For quick tests, run: python -m pytest tests/fast Documentation To re-generate the documentation after the source code was modified: python scripts/render_references.py To run the docs locally, run: mkdocs serve -t material Once ready, you can build it: mkdocs build -t material Or release it on GitHub Pages: mkdocs gh-deploy -t material","title":"Contributions"},{"location":"contributing/#contributions","text":"If you wish to contribute to the project, please do the following: 1. Verify if there's an existing similar issue. 2. If no issue exists, create it. 3. Once the contribution has been discussed inside the issue, fork this repo. 4. Before modifying any code, make sure to read the sections below. 5. Once you are done with your contribution, start a PR and tag a codeowner.","title":"Contributions"},{"location":"contributing/#setup","text":"To set up the development environment, clone the repo: git clone https://github.com/xhlulu/dl-translate cd dl-translate Create a new venv and install the dev dependencies python -m venv venv source venv/bin/activate pip install -e . [ dev ]","title":"Setup"},{"location":"contributing/#code-linting","text":"To ensure consistent and readable code, we use black . To run: python black .","title":"Code linting"},{"location":"contributing/#running-tests","text":"To run all the tests: python -m pytest tests For quick tests, run: python -m pytest tests/fast","title":"Running tests"},{"location":"contributing/#documentation","text":"To re-generate the documentation after the source code was modified: python scripts/render_references.py To run the docs locally, run: mkdocs serve -t material Once ready, you can build it: mkdocs build -t material Or release it on GitHub Pages: mkdocs gh-deploy -t material","title":"Documentation"},{"location":"references/","text":"API Reference dlt.TranslationModel init dlt . TranslationModel . __init__ ( self , model_or_path : str = 'm2m100' , tokenizer_path : str = None , device : str = 'auto' , model_family : str = None , model_options : dict = None , tokenizer_options : dict = None ) Instantiates a multilingual transformer model for translation. Parameter Type Default Description model_or_path str m2m100 The path or the name of the model. Equivalent to the first argument of AutoModel.from_pretrained() . You can also specify shorthands (\"mbart50\" and \"m2m100\"). tokenizer_path str optional The path to the tokenizer. By default, it will be set to model_or_path . device str auto \"cpu\", \"gpu\" or \"auto\". If it's set to \"auto\", will try to select a GPU when available or else fall back to CPU. model_family str optional Either \"mbart50\" or \"m2m100\". By default, it will be inferred based on model_or_path . Needs to be explicitly set if model_or_path is a path. model_options dict optional The keyword arguments passed to the model, which is a transformer for conditional generation. tokenizer_options dict optional The keyword arguments passed to the model's tokenizer. translate dlt . TranslationModel . translate ( self , text : Union [ str , List [ str ]], source : str , target : str , batch_size : int = 32 , verbose : bool = False , generation_options : dict = None ) -> Union [ str , List [ str ]] Translates a string or a list of strings from a source to a target language. Parameter Type Default Description text Union[str, List[str]] required The content you want to translate. source str required The language of the original text. target str required The language of the translated text. batch_size int 32 The number of samples to load at once. If set to None , it will process everything at once. verbose bool False Whether to display the progress bar for every batch processed. generation_options dict optional The keyword arguments passed to model.generate() , where model is the underlying transformers model. Note: - Run print(dlt.utils.available_languages()) to see what's available. - A smaller value is preferred for batch_size if your (video) RAM is limited. get_transformers_model dlt . TranslationModel . get_transformers_model ( self ) Retrieve the underlying mBART transformer model. get_tokenizer dlt . TranslationModel . get_tokenizer ( self ) Retrieve the mBART huggingface tokenizer. available_codes dlt . TranslationModel . available_codes ( self ) -> List [ str ] Returns all the available codes for a given dlt.TranslationModel instance. available_languages dlt . TranslationModel . available_languages ( self ) -> List [ str ] Returns all the available languages for a given dlt.TranslationModel instance. get_lang_code_map dlt . TranslationModel . get_lang_code_map ( self ) -> Dict [ str , str ] Returns the language -> codes dictionary for a given dlt.TranslationModel instance. save_obj dlt . TranslationModel . save_obj ( self , path : str = 'saved_model' ) -> None Saves your model as a torch object and save your tokenizer. Parameter Type Default Description path str saved_model The directory where you want to save your model and tokenizer load_obj dlt . TranslationModel . load_obj ( path : str = 'saved_model' , ** kwargs ) Initialize dlt.TranslationModel from the torch object and tokenizer saved with dlt.TranslationModel.save_obj Parameter Type Default Description path str saved_model The directory where your torch model and tokenizer are stored dlt.utils get_lang_code_map dlt . utils . get_lang_code_map ( weights : str = 'mbart50' ) -> Dict [ str , str ] Get a dictionary mapping a language -> code for a given model. The code will depend on the model you choose. Parameter Type Default Description weights str mbart50 The name of the model you are using. For example, \"mbart50\" is the multilingual BART Large with 50 languages available to use. available_codes dlt . utils . available_codes ( weights : str = 'mbart50' ) -> List [ str ] Get all the codes available for a given model. The code format will depend on the model you select. Parameter Type Default Description weights str mbart50 The name of the model you are using. For example, \"mbart50\" is the multilingual BART Large with 50 codes available to use. available_languages dlt . utils . available_languages ( weights : str = 'mbart50' ) -> List [ str ] Get all the languages available for a given model. Parameter Type Default Description weights str mbart50 The name of the model you are using. For example, \"mbart50\" is the multilingual BART Large with 50 languages available to use.","title":"API Reference"},{"location":"references/#api-reference","text":"","title":"API Reference"},{"location":"references/#dlttranslationmodel","text":"","title":"dlt.TranslationModel"},{"location":"references/#init","text":"dlt . TranslationModel . __init__ ( self , model_or_path : str = 'm2m100' , tokenizer_path : str = None , device : str = 'auto' , model_family : str = None , model_options : dict = None , tokenizer_options : dict = None ) Instantiates a multilingual transformer model for translation. Parameter Type Default Description model_or_path str m2m100 The path or the name of the model. Equivalent to the first argument of AutoModel.from_pretrained() . You can also specify shorthands (\"mbart50\" and \"m2m100\"). tokenizer_path str optional The path to the tokenizer. By default, it will be set to model_or_path . device str auto \"cpu\", \"gpu\" or \"auto\". If it's set to \"auto\", will try to select a GPU when available or else fall back to CPU. model_family str optional Either \"mbart50\" or \"m2m100\". By default, it will be inferred based on model_or_path . Needs to be explicitly set if model_or_path is a path. model_options dict optional The keyword arguments passed to the model, which is a transformer for conditional generation. tokenizer_options dict optional The keyword arguments passed to the model's tokenizer.","title":"init"},{"location":"references/#translate","text":"dlt . TranslationModel . translate ( self , text : Union [ str , List [ str ]], source : str , target : str , batch_size : int = 32 , verbose : bool = False , generation_options : dict = None ) -> Union [ str , List [ str ]] Translates a string or a list of strings from a source to a target language. Parameter Type Default Description text Union[str, List[str]] required The content you want to translate. source str required The language of the original text. target str required The language of the translated text. batch_size int 32 The number of samples to load at once. If set to None , it will process everything at once. verbose bool False Whether to display the progress bar for every batch processed. generation_options dict optional The keyword arguments passed to model.generate() , where model is the underlying transformers model. Note: - Run print(dlt.utils.available_languages()) to see what's available. - A smaller value is preferred for batch_size if your (video) RAM is limited.","title":"translate"},{"location":"references/#get_transformers_model","text":"dlt . TranslationModel . get_transformers_model ( self ) Retrieve the underlying mBART transformer model.","title":"get_transformers_model"},{"location":"references/#get_tokenizer","text":"dlt . TranslationModel . get_tokenizer ( self ) Retrieve the mBART huggingface tokenizer.","title":"get_tokenizer"},{"location":"references/#available_codes","text":"dlt . TranslationModel . available_codes ( self ) -> List [ str ] Returns all the available codes for a given dlt.TranslationModel instance.","title":"available_codes"},{"location":"references/#available_languages","text":"dlt . TranslationModel . available_languages ( self ) -> List [ str ] Returns all the available languages for a given dlt.TranslationModel instance.","title":"available_languages"},{"location":"references/#get_lang_code_map","text":"dlt . TranslationModel . get_lang_code_map ( self ) -> Dict [ str , str ] Returns the language -> codes dictionary for a given dlt.TranslationModel instance.","title":"get_lang_code_map"},{"location":"references/#save_obj","text":"dlt . TranslationModel . save_obj ( self , path : str = 'saved_model' ) -> None Saves your model as a torch object and save your tokenizer. Parameter Type Default Description path str saved_model The directory where you want to save your model and tokenizer","title":"save_obj"},{"location":"references/#load_obj","text":"dlt . TranslationModel . load_obj ( path : str = 'saved_model' , ** kwargs ) Initialize dlt.TranslationModel from the torch object and tokenizer saved with dlt.TranslationModel.save_obj Parameter Type Default Description path str saved_model The directory where your torch model and tokenizer are stored","title":"load_obj"},{"location":"references/#dltutils","text":"","title":"dlt.utils"},{"location":"references/#get_lang_code_map_1","text":"dlt . utils . get_lang_code_map ( weights : str = 'mbart50' ) -> Dict [ str , str ] Get a dictionary mapping a language -> code for a given model. The code will depend on the model you choose. Parameter Type Default Description weights str mbart50 The name of the model you are using. For example, \"mbart50\" is the multilingual BART Large with 50 languages available to use.","title":"get_lang_code_map"},{"location":"references/#available_codes_1","text":"dlt . utils . available_codes ( weights : str = 'mbart50' ) -> List [ str ] Get all the codes available for a given model. The code format will depend on the model you select. Parameter Type Default Description weights str mbart50 The name of the model you are using. For example, \"mbart50\" is the multilingual BART Large with 50 codes available to use.","title":"available_codes"},{"location":"references/#available_languages_1","text":"dlt . utils . available_languages ( weights : str = 'mbart50' ) -> List [ str ] Get all the languages available for a given model. Parameter Type Default Description weights str mbart50 The name of the model you are using. For example, \"mbart50\" is the multilingual BART Large with 50 languages available to use.","title":"available_languages"}]}